{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "bCIMTPB1WoTq",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7yVV6txOmNMn"
   },
   "source": [
    "# Introduction to Long Context Window with Gemini on Vertex AI\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/long-context/intro_long_context.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Flong-context%2Fintro_long_context.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
    "    </a>\n",
    "  </td>    \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/long-context/intro_long_context.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/long-context/intro_long_context.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1EExYZvij2ve"
   },
   "source": [
    "| | |\n",
    "|-|-|\n",
    "|Author(s) | [Holt Skinner](https://github.com/holtskinner) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t1DnOs6rkbOy"
   },
   "source": [
    "## Overview\n",
    "\n",
    "Gemini 1.5 Flash comes standard with a 1 million token context window, and Gemini 1.5 Pro comes with a 2 million token context window. Historically, large language models (LLMs) were significantly limited by the amount of text (or tokens) that could be passed to the model at one time. The Gemini 1.5 long context window, with [near-perfect retrieval (>99%)](https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf), unlocks many new use cases and developer paradigms.\n",
    "\n",
    "In practice, 1 million tokens would look like:\n",
    "\n",
    "-   50,000 lines of code (with the standard 80 characters per line)\n",
    "-   All the text messages you have sent in the last 5 years\n",
    "-   8 average length English novels\n",
    "-   Transcripts of over 200 average length podcast episodes\n",
    "-   1 hour of video\n",
    "-   ~45 minutes of video with audio\n",
    "-   9.5 hours of audio\n",
    "\n",
    "While the standard use case for most generative models is still text input, the Gemini 1.5 model family enables a new paradigm of multimodal use cases. These models can natively understand text, video, audio, and images.\n",
    "\n",
    "In this notebook, we'll explore multimodal use cases of the long context window.\n",
    "\n",
    "For more information, refer to the [Gemini documentation about long context](https://ai.google.dev/gemini-api/docs/long-context)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3e127df191a2"
   },
   "source": [
    "## Tokens\n",
    "\n",
    "Tokens can be single characters like `z` or whole words like `cat`. Long words\n",
    "are broken up into several tokens. The set of all tokens used by the model is\n",
    "called the vocabulary, and the process of splitting text into tokens is called\n",
    "_tokenization_.\n",
    "\n",
    "> **Important:** For Gemini models, a token is equivalent to about 4 characters. 100 tokens is equal to about 60-80 English words.\n",
    "\n",
    "For multimodal input, this is how tokens are calculated regardless of display or file size:\n",
    "\n",
    "* Images: `258` tokens\n",
    "* Video: `263` tokens per second\n",
    "* Audio: `32` tokens per second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a8aa3bd0feda"
   },
   "source": [
    "## Why is the long context window useful?\n",
    "\n",
    "The basic way you use the Gemini models is by passing information (context)\n",
    "to the model, which will subsequently generate a response. An analogy for the\n",
    "context window is short term memory. There is a limited amount of information\n",
    "that can be stored in someone's short term memory, and the same is true for\n",
    "generative models.\n",
    "\n",
    "You can read more about how models work under the hood in our [generative models guide](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/overview).\n",
    "\n",
    "Even though the models can take in more and more context, much of the\n",
    "conventional wisdom about using large language models assumes this inherent\n",
    "limitation on the model, which as of 2024, is no longer the case.\n",
    "\n",
    "Some common strategies to handle the limitation of small context windows\n",
    "included:\n",
    "\n",
    "-   Arbitrarily dropping old messages / text from the context window as new text\n",
    "    comes in\n",
    "-   Summarizing previous content and replacing it with the summary when the\n",
    "    context window gets close to being full\n",
    "-   Using RAG with semantic search to move data out of the context window and\n",
    "    into a vector database\n",
    "-   Using deterministic or generative filters to remove certain text /\n",
    "    characters from prompts to save tokens\n",
    "\n",
    "While many of these are still relevant in certain cases, the default place to start is now just putting all of the tokens into the context window. Because Gemini 1.5 models were purpose-built with a long context window, they are much more capable of in-context learning. This means that instructional materials provided in context can be highly effective for handling inputs that are not covered by the model's training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61RBz8LLbxCR"
   },
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "No17Cw5hgx12"
   },
   "source": [
    "### Install Vertex AI SDK for Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "tFy3H3aPgx12",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m  WARNING: The script tb-gcp-uploader is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --user --quiet google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5Xep4W9lq-Z"
   },
   "source": [
    "### Restart runtime\n",
    "\n",
    "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which restarts the current kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "XRvKdaPDTznN",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbmM4z7FOBpM"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ The kernel is going to restart. Please wait until it is finished before continuing to the next step. ⚠️</b>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmWOrTJ3gx13"
   },
   "source": [
    "### Authenticate your notebook environment (Colab only)\n",
    "\n",
    "If you are running this notebook on Google Colab, run the cell below to authenticate your environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "NyKGtVQjgx13",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DF4l8DTdWgPY"
   },
   "source": [
    "### Set Google Cloud project information and initialize Vertex AI SDK\n",
    "\n",
    "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
    "\n",
    "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Nqwi-5ufWp_B",
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"qwiklabs-gcp-02-9c890bf5875d\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
    "\n",
    "import vertexai\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jXHfaVS66_01"
   },
   "source": [
    "### Import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "lslYAvw37JGQ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "from vertexai.generative_models import GenerationConfig, GenerativeModel, Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BY1nfXrqRxVX"
   },
   "source": [
    "### Load the Gemini 1.5 Flash model\n",
    "\n",
    "To learn more about all [Gemini API models on Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-models).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "U7ExWmuLBdIA",
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_ID = \"gemini-1.5-flash\"  # @param {type:\"string\"}\n",
    "\n",
    "model = GenerativeModel(\n",
    "    MODEL_ID, generation_config=GenerationConfig(max_output_tokens=8192)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l9OKM0-4SQf8"
   },
   "source": [
    "## Long-form text\n",
    "\n",
    "Text has proved to be the layer of intelligence underpinning much of the momentum around LLMs. As mentioned earlier, much of the practical limitation of LLMs was because of not having a large enough context window to do certain tasks. This led to the rapid adoption of retrieval augmented generation (RAG) and other techniques which dynamically provide the model with relevant\n",
    "contextual information.\n",
    "\n",
    "Some emerging and standard use cases for text based long context include:\n",
    "\n",
    "-   Summarizing large corpuses of text\n",
    "    -   Previous summarization options with smaller context models would require\n",
    "        a sliding window or another technique to keep state of previous sections\n",
    "        as new tokens are passed to the model\n",
    "-   Question and answering\n",
    "    -   Historically this was only possible with RAG given the limited amount of\n",
    "        context and models' factual recall being low\n",
    "-   Agentic workflows\n",
    "    -   Text is the underpinning of how agents keep state of what they have done\n",
    "        and what they need to do; not having enough information about the world\n",
    "        and the agent's goal is a limitation on the reliability of agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2dab25e392cb"
   },
   "source": [
    "[War and Peace by Leo Tolstoy](https://en.wikipedia.org/wiki/War_and_Peace) is considered one of the greatest literary works of all time; however, it is over 1,225 pages and the average reader will spend 37 hours and 48 minutes reading this book at 250 WPM (words per minute). 😵‍💫 The text alone takes up 3.4 MB of storage space. However, the entire novel consists of less than 900,000 tokens, so it will fit within the Gemini context window.\n",
    "\n",
    "We are going to pass in the entire text into Gemini 1.5 Flash and get a detailed summary of the plot. For this example, we have the text of the novel from [Project Gutenberg](https://www.gutenberg.org/ebooks/2600) stored in a public Google Cloud Storage bucket.\n",
    "\n",
    "First, we will use the `count_tokens()` method to examine the token count of the full prompt, then send the prompt to Gemini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "FhFxrtfdSwOP",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_tokens: 839583\n",
      "total_billable_characters: 43\n",
      "prompt_tokens_details {\n",
      "  modality: TEXT\n",
      "  token_count: 839583\n",
      "}\n",
      "\n",
      "\n",
      "Usage metadata:\n",
      "prompt_token_count: 839583\n",
      "candidates_token_count: 1746\n",
      "total_token_count: 841329\n",
      "prompt_tokens_details {\n",
      "  modality: TEXT\n",
      "  token_count: 839583\n",
      "}\n",
      "candidates_tokens_details {\n",
      "  modality: TEXT\n",
      "  token_count: 1746\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## War and Peace: A Detailed Summary\n",
       "\n",
       "Leo Tolstoy's epic novel, *War and Peace*, is a sprawling tale of love, loss, and the impact of war on Russian society during the Napoleonic era. It follows the intertwined lives of several aristocratic families, particularly the Rostovs, the Bolkonskys, and the Bezukhovs, as they navigate personal challenges and the upheaval of the 1805 and 1812 wars.\n",
       "\n",
       "**Part One:**\n",
       "\n",
       "The novel opens in 1805, amidst the societal and political tensions leading up to Napoleon's invasion of Russia. We are introduced to a vibrant cast of characters:\n",
       "\n",
       "* **The Rostovs:** A loving and boisterous family who embody the traditional Russian way of life, focused on family, social events, and military service. The count, Ilyá, is a jovial, generous, and slightly frivolous man. The countess, Nataly, is a caring and protective mother. Their children, Nicholas, Natásha, and Pétya, are spirited and full of life.\n",
       "* **The Bolkonskys:** A family of intellectual and proud aristocrats, deeply rooted in traditional values and service to the Tsar. Prince Nicholas, a stern and demanding man, is the father of Prince Andrew, a disillusioned and contemplative military man, and Princess Mary, a pious and gentle woman.\n",
       "* **The Bezukhovs:** This family is defined by wealth, intrigue, and loose morals. Count Bezúkhov, a wealthy and powerful old man, is dying, leaving his vast fortune and his illegitimate son, Pierre, to the whims of ambitious relatives. \n",
       "\n",
       "The first part explores the characters' individual journeys:\n",
       "\n",
       "* **Prince Andrew:**  Disillusioned with society and his marriage, he seeks meaning in military service. He develops a strong respect for Napoleon, but also feels a growing sense of alienation and despair.\n",
       "* **Natásha:**  A spirited, mischievous, and passionate girl, she explores her first love, but remains mostly unfazed by the social expectations surrounding her.\n",
       "* **Pierre:**  Awkward and introspective, he searches for meaning in various pursuits, including philosophy and the pursuit of a moral life. \n",
       "\n",
       "**Part Two:**\n",
       "\n",
       "The second part focuses on the early stages of the war against Napoleon. The Russian army, led by General Kutúzov, is in a precarious position, facing a superior French force.  \n",
       "\n",
       "* **Prince Andrew:** Serving on Kutúzov’s staff, he experiences the complexities of war and the shortcomings of the Austrian alliance. He encounters a sense of hopelessness and a desire for something more meaningful.\n",
       "* **Nicholas:** Joining the Pávlograd Hussars, he experiences his first taste of battlefield action and learns to navigate the chaos of war. He witnesses Dólokhov’s reckless behavior and experiences the consequences of impulsive decisions.\n",
       "* **Pierre:**  He struggles to find his place in the midst of war, ultimately finding a sense of purpose and meaning in the simplicity of the soldiers he encounters. \n",
       "\n",
       "**Part Three:**\n",
       "\n",
       "This part focuses on the pivotal battle of Austerlitz, a major defeat for the Russian and Austrian alliance. Prince Andrew is wounded and experiences a spiritual awakening as he gazes at the sky, recognizing the futility of war and the importance of something beyond earthly matters.\n",
       "\n",
       "* **Natásha:** At a Moscow ball, she is captivated by the beauty of society and the possibility of love and romance.\n",
       "* **Pierre:**  He becomes enamored by Hélène, and despite his initial misgivings, eventually marries her.\n",
       "\n",
       "**Part Four:**\n",
       "\n",
       "Now a rich man, Pierre struggles with his newfound wealth and responsibility, grappling with his wife’s infidelity and the corruption of the society around him. He finds solace in Freemasonry and seeks a more meaningful life through philanthropy and self-improvement. \n",
       "\n",
       "**Part Five:**\n",
       "\n",
       "The novel shifts focus to the war against Prussia and Napoleon. Prince Andrew, now disillusioned with military service and unable to find fulfillment in the upper circles of Petersburg society, seeks refuge in the country and dedicates himself to improving the lives of his serfs. \n",
       "\n",
       "**Part Six:**\n",
       "\n",
       "This part explores the personal struggles and transformations of the characters, with a focus on:\n",
       "\n",
       "* **Natásha:** She experiences heartbreak and despair after the death of her mother and grapples with the loss of her youthful innocence.\n",
       "* **Prince Andrew:** He finds new purpose in caring for his son, reconnecting with his family, and exploring his spiritual and philosophical interests.\n",
       "* **Pierre:**  He continues his pursuit of a more fulfilling life through Freemasonry and philanthropic endeavors.\n",
       "\n",
       "**Part Seven:**\n",
       "\n",
       "The Russian army prepares for a fresh confrontation with Napoleon. The tension and complexity of the war machine are portrayed, with conflicting opinions and strategies emerging among the military leaders. Prince Andrew is determined to contribute to the defense of his country.\n",
       "\n",
       "**Part Eight:**\n",
       "\n",
       "The invasion begins. The Russian army, initially disorganized and lacking clear leadership, experiences the horrors of the war as they retreat from the French advance. \n",
       "\n",
       "* **Nicholas:** He faces challenges, including financial woes and his growing feelings for Sónya. \n",
       "* **Prince Andrew:** He seeks a role in the war, and ultimately rejoins the Russian army. \n",
       "* **Pierre:** He feels a renewed sense of purpose and a desire to sacrifice himself for a greater good.\n",
       "\n",
       "**Part Nine:**\n",
       "\n",
       "The battle of Borodinó is fought, a pivotal event that shifts the momentum of the war.\n",
       "\n",
       "* **Nicholas:**  He experiences the horrors of the battlefield and the consequences of war.\n",
       "* **Prince Andrew:** He encounters Napoleon, and has a vision of the infinite, but is fatally wounded.\n",
       "* **Pierre:** He witnesses the horrors of war firsthand, and seeks meaning amidst the chaos. \n",
       "\n",
       "**Part Ten:**\n",
       "\n",
       "The French advance to Moscow, and the city is occupied and burned down. The\n",
       "impact of these events on the characters is profound.\n",
       "\n",
       "* **Natásha:** She is devastated by the loss of her brother, Pétya, and falls ill with despair.\n",
       "* **Pierre:** He embarks on a journey of self-discovery, ultimately seeking solace and truth in the simple wisdom of a Russian peasant, Platón Karatáev.\n",
       "\n",
       "**Part Eleven:**\n",
       "\n",
       "The Russian army re-groups and the French are forced to retreat, leading to the battle of Tarútino. \n",
       "\n",
       "* **Prince Andrew:**  He dies in the arms of Natásha, who is dedicated to caring for him. \n",
       "\n",
       "**Part Twelve:**\n",
       "\n",
       "The French retreat continues. The Russian army pursues them, and the prisoners they capture, including Pierre, face the brutal realities of war and captivity.  Pierre finds peace and a new sense of life through his interactions with Platón Karatáev.\n",
       "\n",
       "**Part Thirteen:**\n",
       "\n",
       "The French army is in full retreat. The Russian army pursues them, but the pursuit is hindered by the exhaustion of the troops.\n",
       "\n",
       "* **Nicholas:** He seeks to distinguish himself in the war, but faces moral dilemmas and financial difficulties. \n",
       "* **Princess Mary:** She grapples with the loss of her father and her love for Nicholas. \n",
       "* **Pierre:** He is imprisoned and witnesses the brutality of the French army.\n",
       "\n",
       "**Part Fourteen:**\n",
       "\n",
       "The final phase of the war, characterized by the relentless pursuit of\n",
       "the French. \n",
       "\n",
       "* **Pierre:** He is liberated from captivity and joins the Russian army.\n",
       "* **Nicholas:** He is reunited with Sónya, and experiences a profound shift in his feelings for her.\n",
       "\n",
       "**Part Fifteen:**\n",
       "\n",
       "The war comes to an end. \n",
       "\n",
       "* **Nicholas:** He marries Princess Mary, and finds happiness in family life and his passion for farming.\n",
       "* **Natásha:** She eventually marries Pierre, finding solace and a new sense of purpose in family life. \n",
       "\n",
       "**Epilogue:**\n",
       "\n",
       "The novel concludes with reflections on the nature of power, war, and the\n",
       "meaning of life. It emphasizes the importance of compassion, faith, and\n",
       "the simple virtues of family and home. \n",
       "\n",
       "*War and Peace* is a complex and nuanced exploration of human\n",
       "nature, love, and the destructive forces of war. It is not a novel\n",
       "that offers easy answers, but rather invites readers to contemplate\n",
       "the intricacies of life, the complexity of human relationships, and\n",
       "the enduring power of love and faith. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set contents to send to the model\n",
    "contents = [\n",
    "    \"Provide a detailed summary of the following novel.\",\n",
    "    Part.from_uri(\n",
    "        \"gs://github-repo/generative-ai/gemini/long-context/WarAndPeace.txt\",\n",
    "        mime_type=\"text/plain\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Counts tokens\n",
    "print(model.count_tokens(contents))\n",
    "\n",
    "# Prompt the model to generate content\n",
    "response = model.generate_content(\n",
    "    contents,\n",
    ")\n",
    "\n",
    "# Print the model response\n",
    "print(f\"\\nUsage metadata:\\n{response.usage_metadata}\")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bedc5fe7ac33"
   },
   "source": [
    "## Long-form video\n",
    "\n",
    "Video content has been difficult to process due to constraints of the format itself.\n",
    "It was hard to skim the content, transcripts often failed to capture the nuance of a video, and most tools don't process images, text, and audio together.\n",
    "The Gemini 1.5 long context window allows the ability to reason and answer questions about multimodal inputs with\n",
    "sustained performance.\n",
    "\n",
    "When tested on the needle in a video haystack problem with 1M tokens, Gemini 1.5 Flash obtained >99.8% recall of the video in the context window, and Gemini 1.5 Pro reached state of the art performance on the [Video-MME benchmark](https://video-mme.github.io/home_page.html).\n",
    "\n",
    "Some emerging and standard use cases for video long context include:\n",
    "\n",
    "-   Video question and answering\n",
    "-   Video memory, as shown with [Google's Project Astra](https://deepmind.google/technologies/gemini/project-astra/)\n",
    "-   Video captioning\n",
    "-   Video recommendation systems, by enriching existing metadata with new\n",
    "    multimodal understanding\n",
    "-   Video customization, by looking at a corpus of data and associated video\n",
    "    metadata and then removing parts of videos that are not relevant to the\n",
    "    viewer\n",
    "-   Video content moderation\n",
    "-   Real-time video processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14c6c9427a81"
   },
   "source": [
    "[Google I/O](https://io.google/) is one of the major events when Google's developer tools are announced. Workshop sessions and are filled with a lot of material, so it can be difficult to keep track all that is discussed.\n",
    "\n",
    "We are going to use a video of a session from Google I/O 2024 focused on [Grounding for Gemini](https://www.youtube.com/watch?v=v4s5eU2tfd4) to calculate tokens and process the information presented. We will ask a specific question about a point in the video and ask for a general summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "c7890cf45808",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_tokens: 607064\n",
      "total_billable_characters: 54\n",
      "prompt_tokens_details {\n",
      "  modality: VIDEO\n",
      "  token_count: 607050\n",
      "}\n",
      "prompt_tokens_details {\n",
      "  modality: TEXT\n",
      "  token_count: 14\n",
      "}\n",
      "\n",
      "\n",
      "Usage metadata:\n",
      "prompt_token_count: 607064\n",
      "candidates_token_count: 20\n",
      "total_token_count: 607084\n",
      "prompt_tokens_details {\n",
      "  modality: VIDEO\n",
      "  token_count: 607050\n",
      "}\n",
      "prompt_tokens_details {\n",
      "  modality: TEXT\n",
      "  token_count: 14\n",
      "}\n",
      "candidates_tokens_details {\n",
      "  modality: TEXT\n",
      "  token_count: 20\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The Cymbal Starlight demo is at **24:53** in the video. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set contents to send to the model\n",
    "video = Part.from_uri(\n",
    "    \"gs://github-repo/generative-ai/gemini/long-context/GoogleIOGroundingRAG.mp4\",\n",
    "    mime_type=\"video/mp4\",\n",
    ")\n",
    "\n",
    "contents = [\"At what time in the following video is the Cymbal Starlight demo?\", video]\n",
    "\n",
    "# Counts tokens\n",
    "print(model.count_tokens(contents))\n",
    "\n",
    "# Prompt the model to generate content\n",
    "response = model.generate_content(\n",
    "    contents,\n",
    ")\n",
    "\n",
    "# Print the model response\n",
    "print(f\"\\nUsage metadata:\\n{response.usage_metadata}\")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "d5b00e40bd9f",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_tokens: 607063\n",
      "total_billable_characters: 69\n",
      "prompt_tokens_details {\n",
      "  modality: TEXT\n",
      "  token_count: 13\n",
      "}\n",
      "prompt_tokens_details {\n",
      "  modality: VIDEO\n",
      "  token_count: 607050\n",
      "}\n",
      "\n",
      "\n",
      "Usage metadata:\n",
      "prompt_token_count: 607063\n",
      "candidates_token_count: 167\n",
      "total_token_count: 607230\n",
      "prompt_tokens_details {\n",
      "  modality: TEXT\n",
      "  token_count: 13\n",
      "}\n",
      "prompt_tokens_details {\n",
      "  modality: VIDEO\n",
      "  token_count: 607050\n",
      "}\n",
      "candidates_tokens_details {\n",
      "  modality: TEXT\n",
      "  token_count: 167\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Hey developers! This video is a must-watch if you want to level up your AI game. Get ready to learn about ground systems for Gemini using Vertex AI search and some DIY RAG techniques. The speaker, Holt Skinner, is a developer advocate for Google Cloud and he breaks down everything you need to know about RAG – Retrieval-Augmented Generation. You'll discover how to ground your large language models on real-world data, make your systems more accurate and up-to-date, and avoid pesky hallucinations. Plus, learn about different approaches to building a DIY RAG system, using both Google Search and Vertex AI vector search.  Don't miss out on this insightful breakdown of powerful tools that will help you build smarter, more informative AI applications.  Get ready to take your AI skills to the next level! \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "contents = [\n",
    "    \"Provide an enthusiastic summary of the video, tailored for software developers.\",\n",
    "    video,\n",
    "]\n",
    "\n",
    "# Counts tokens\n",
    "print(model.count_tokens(contents))\n",
    "\n",
    "# Prompt the model to generate content\n",
    "response = model.generate_content(contents)\n",
    "\n",
    "# Print the model response\n",
    "print(f\"\\nUsage metadata:\\n{response.usage_metadata}\")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a5d0d710941c"
   },
   "source": [
    "## Long-form audio\n",
    "\n",
    "In order to process audio, developers have typically needed to string together multiple models, like a speech-to-text model and a text-to-text model, in order to process audio. This led to additional latency due to multiple round-trip requests, and the context of the audio itself could be lost.\n",
    "\n",
    "The Gemini 1.5 models were the first natively multimodal large language models that could understand audio.\n",
    "\n",
    "On standard audio-haystack evaluations, Gemini 1.5 Pro is able to find the hidden audio in 100% of the tests and Gemini 1.5 Flash is able to find it in 98.7% [of the tests](https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf). Further, on a test set of 15-minute audio clips, Gemini 1.5 Pro archives a word error rate (WER) of ~5.5%, much lower than even specialized speech-to-text models, without the added complexity of extra input segmentation and pre-processing.\n",
    "\n",
    "The long context window accepts up to 9.5 hours of audio in a single request.\n",
    "\n",
    "Some emerging and standard use cases for audio context include:\n",
    "\n",
    "-   Real-time transcription and translation\n",
    "-   Podcast / video question and answering\n",
    "-   Meeting transcription and summarization\n",
    "-   Voice assistants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7d3bb89a87a5"
   },
   "source": [
    "Podcasts are a great way to learn about the latest news in technology, but there are so many out there that it can be difficult to follow them all. It's also challenging to find a specific episode with a given topic or a quote.\n",
    "\n",
    "In this example, we will process 9 episodes of the [Google Kubernetes Podcast](https://cloud.google.com/podcasts/kubernetespodcast) and ask specific questions about the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "672059078485",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_tokens: 843569\n",
      "total_billable_characters: 80\n",
      "prompt_tokens_details {\n",
      "  modality: TEXT\n",
      "  token_count: 19\n",
      "}\n",
      "prompt_tokens_details {\n",
      "  modality: AUDIO\n",
      "  token_count: 843550\n",
      "}\n",
      "\n",
      "\n",
      "Usage metadata:\n",
      "prompt_token_count: 843569\n",
      "candidates_token_count: 74\n",
      "total_token_count: 843643\n",
      "prompt_tokens_details {\n",
      "  modality: TEXT\n",
      "  token_count: 19\n",
      "}\n",
      "prompt_tokens_details {\n",
      "  modality: AUDIO\n",
      "  token_count: 843550\n",
      "}\n",
      "candidates_tokens_details {\n",
      "  modality: TEXT\n",
      "  token_count: 74\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The podcasts discuss how Kubernetes has evolved and become more mature in the last 10 years. They talk about how it has been adopted by a wide range of companies and how it is being used for new types of workloads, such as AI/ML workloads.  They also discuss how the Kubernetes community is trying to make the project more accessible to new contributors. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set contents to send to the model\n",
    "contents = [\n",
    "    \"According to the following podcasts, what can you tell me about AI/ML workloads on Kubernetes?\",\n",
    "    Part.from_uri(\n",
    "        \"gs://github-repo/generative-ai/gemini/long-context/20240417-kpod223.mp3\",\n",
    "        mime_type=\"audio/mpeg\",\n",
    "    ),\n",
    "    Part.from_uri(\n",
    "        \"gs://github-repo/generative-ai/gemini/long-context/20240430-kpod224.mp3\",\n",
    "        mime_type=\"audio/mpeg\",\n",
    "    ),\n",
    "    Part.from_uri(\n",
    "        \"gs://github-repo/generative-ai/gemini/long-context/20240515-kpod225.mp3\",\n",
    "        mime_type=\"audio/mpeg\",\n",
    "    ),\n",
    "    Part.from_uri(\n",
    "        \"gs://github-repo/generative-ai/gemini/long-context/20240529-kpod226.mp3\",\n",
    "        mime_type=\"audio/mpeg\",\n",
    "    ),\n",
    "    Part.from_uri(\n",
    "        \"gs://github-repo/generative-ai/gemini/long-context/20240606-kpod227.mp3\",\n",
    "        mime_type=\"audio/mpeg\",\n",
    "    ),\n",
    "    Part.from_uri(\n",
    "        \"gs://github-repo/generative-ai/gemini/long-context/20240611-kpod228.mp3\",\n",
    "        mime_type=\"audio/mpeg\",\n",
    "    ),\n",
    "    Part.from_uri(\n",
    "        \"gs://github-repo/generative-ai/gemini/long-context/20240625-kpod229.mp3\",\n",
    "        mime_type=\"audio/mpeg\",\n",
    "    ),\n",
    "    Part.from_uri(\n",
    "        \"gs://github-repo/generative-ai/gemini/long-context/20240709-kpod230.mp3\",\n",
    "        mime_type=\"audio/mpeg\",\n",
    "    ),\n",
    "    Part.from_uri(\n",
    "        \"gs://github-repo/generative-ai/gemini/long-context/20240723-kpod231.mp3\",\n",
    "        mime_type=\"audio/mpeg\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Counts tokens\n",
    "print(model.count_tokens(contents))\n",
    "\n",
    "# Prompt the model to generate content\n",
    "response = model.generate_content(\n",
    "    contents,\n",
    ")\n",
    "\n",
    "# Print the model response\n",
    "print(f\"\\nUsage metadata:\\n{response.usage_metadata}\")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8291d9972287"
   },
   "source": [
    "## Code\n",
    "\n",
    "For a long context window use case involving ingesting an entire GitHub repository, check out [Analyze a codebase with Vertex AI Gemini 1.5 Pro](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/code/analyze_codebase_with_gemini_1_5_pro.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aec8e711926b"
   },
   "source": [
    "## Context caching\n",
    "\n",
    "[Context caching](https://cloud.google.com/vertex-ai/generative-ai/docs/context-cache/context-cache-overview) allows developers to reduce the time and cost of repeated requests using the large context window.\n",
    "For examples on how to use Context Caching with Gemini on Vertex AI, refer to [Intro to Context Caching with Gemini on Vertex AI](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/context-caching/intro_context_caching.ipynb)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "intro_long_context.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m128",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m128"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
